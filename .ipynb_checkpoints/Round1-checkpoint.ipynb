{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40419f4a-ef5c-4075-815a-1cf0801a029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Missing %': np.float64(16.26), 'Duplicate %': np.float64(0.0), 'Outlier %': np.float64(1.11), 'Purity Score': np.float64(93.28), 'Data_cleaning_score': -10}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dataset_purity(df):\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "\n",
    "    # 1. Missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    missing_pct = (missing_count / total_cells) * 100\n",
    "\n",
    "    # 2. Duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    duplicate_pct = (duplicate_count / len(df)) * 100 if len(df) > 0 else 0\n",
    "\n",
    "    # 3. Data type consistency (basic check)\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    dtype_consistency = ((len(numeric_cols) + len(cat_cols)) / df.shape[1]) * 100 if df.shape[1] > 0 else 0\n",
    "\n",
    "    # 4. Outlier detection (IQR method for numeric cols)\n",
    "    outlier_count = 0\n",
    "    total_numeric = 0\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_count += ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        total_numeric += len(df[col])\n",
    "    outlier_pct = (outlier_count / total_numeric) * 100 if total_numeric > 0 else 0\n",
    "\n",
    "    # Combine into purity score (weights adjustable)\n",
    "    purity_score = 100 - (0.4*missing_pct + 0.3*duplicate_pct + 0.2*outlier_pct)\n",
    "\n",
    "    # FIX: removed commas that made these tuples instead of numbers\n",
    "    Missing = round(missing_pct, 2)\n",
    "    Duplicate = round(duplicate_pct, 2)\n",
    "    Outlier = round(outlier_pct, 2)\n",
    "    Purity = max(0, round(purity_score, 2))\n",
    "\n",
    "    Data_cleaning_score = int(0)\n",
    "    if int(Missing) > 1.11:\n",
    "        Data_cleaning_score -= 20\n",
    "    elif Missing == 0:\n",
    "        Data_cleaning_score += 20\n",
    "\n",
    "    if Duplicate > 0:\n",
    "        Data_cleaning_score -= 20\n",
    "    else:\n",
    "        Data_cleaning_score += 20\n",
    "\n",
    "    if Outlier < 1.11 and Outlier > 0:\n",
    "        Data_cleaning_score += 20\n",
    "    elif Outlier == 0:\n",
    "        Data_cleaning_score += 50\n",
    "    elif Outlier == 1.11:\n",
    "        Data_cleaning_score += 10    \n",
    "    else:\n",
    "        Data_cleaning_score -= 40\n",
    "\n",
    "    if int(Purity) == 100:\n",
    "        print('The Participant is qualified for the next round!')\n",
    "        Data_cleaning_score += 60\n",
    "    elif int(Purity) == 93.28:\n",
    "        Data_cleaning_score += 10\n",
    "    elif int(Purity) > 93.28 and int(Purity) < 100:\n",
    "        Data_cleaning_score += 40\n",
    "    else:\n",
    "        Data_cleaning_score -= 20\n",
    "        \n",
    "    return {\n",
    "        \"Missing %\": round(missing_pct, 2),\n",
    "        \"Duplicate %\": round(duplicate_pct, 2),\n",
    "        \"Outlier %\": round(outlier_pct, 2),\n",
    "        \"Purity Score\": max(0, round(purity_score, 2)),\n",
    "        \"Data_cleaning_score\": int(Data_cleaning_score)\n",
    "    }\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv(\"Nike_Sales_Uncleaned.csv\")\n",
    "print(dataset_purity(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defee200-cb08-4a5b-98c8-093e913fcfb3",
   "metadata": {},
   "source": [
    "Program for evaluation for the First Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "319e54d7-c852-4191-8e69-4c285c2d59df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for handling Missing Values = 20\n",
      "Score for handling Duplicate Values = 20\n",
      "Score for handling Outliers Values = 50\n",
      "Score for handling Purity = 60\n",
      "\n",
      " The Participant is qualified for the next round!\n",
      "Missing %: 0.0\n",
      "Duplicate %: 0.0\n",
      "Outlier %: 0.0\n",
      "Purity Score: 100.0\n",
      "Data_cleaning_score: 150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def dataset_purity(df):\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "\n",
    "    # 1. Missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    missing_pct = (missing_count / total_cells) * 100\n",
    "\n",
    "    # 2. Duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    duplicate_pct = (duplicate_count / len(df)) * 100 if len(df) > 0 else 0\n",
    "\n",
    "    # 3. Data type consistency (basic check)\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    dtype_consistency = ((len(numeric_cols) + len(cat_cols)) / df.shape[1]) * 100 if df.shape[1] > 0 else 0\n",
    "\n",
    "    # 4. Outlier detection (IQR method for numeric cols)\n",
    "    outlier_count = 0\n",
    "    total_numeric = 0\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outlier_count += ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "        total_numeric += len(df[col])\n",
    "    outlier_pct = (outlier_count / total_numeric) * 100 if total_numeric > 0 else 0\n",
    "\n",
    "    # Combine into purity score (weights adjustable)\n",
    "    purity_score = 100 - (0.4*missing_pct + 0.3*duplicate_pct + 0.2*outlier_pct)\n",
    "\n",
    "    # FIX: removed commas that made these tuples instead of numbers\n",
    "    Missing = round(missing_pct, 2)\n",
    "    Duplicate = round(duplicate_pct, 2)\n",
    "    Outlier = round(outlier_pct, 2)\n",
    "    Purity = max(0, round(purity_score, 2))\n",
    "\n",
    "    Data_cleaning_score = int(0)\n",
    "    if int(Missing) > 1.11:\n",
    "        Data_cleaning_score -= 20\n",
    "        print(\"Score for handling Missing Values = -20\")\n",
    "    elif Missing == 0:\n",
    "        Data_cleaning_score += 20\n",
    "        print(\"Score for handling Missing Values = 20\")\n",
    "    if Duplicate > 0:\n",
    "        Data_cleaning_score -= 20\n",
    "        print(\"Score for handling Duplicate Values = -20\")\n",
    "    else:\n",
    "        Data_cleaning_score += 20\n",
    "        print(\"Score for handling Duplicate Values = 20\")\n",
    "\n",
    "    if Outlier < 1.11 and Outlier > 0:\n",
    "        Data_cleaning_score += 20\n",
    "        print(\"Score for handling Outliers = 20\")\n",
    "    elif Outlier == 0:\n",
    "        Data_cleaning_score += 50\n",
    "        print(\"Score for handling Outliers Values = 50\")\n",
    "    elif Outlier == 1.11:\n",
    "        Data_cleaning_score += 10 \n",
    "        print(\"Score for handling Outliers Values = 10\")\n",
    "    else:\n",
    "        Data_cleaning_score -= 40\n",
    "        print(\"Score for handling Outliers Values = -40\")\n",
    "\n",
    "    if int(Purity) == 100:\n",
    "        \n",
    "        Data_cleaning_score += 60\n",
    "        print(\"Score for handling Purity = 60\")\n",
    "        print('\\n The Participant is qualified for the next round!')\n",
    "    elif int(Purity) == 93.28:\n",
    "        Data_cleaning_score += 10\n",
    "        print(\"Score for handling Purity = 10\")\n",
    "    \n",
    "    elif int(Purity) > 93.28 and int(Purity) < 100:\n",
    "        Data_cleaning_score += 40\n",
    "        print(\"Score for handling Purity = 40\")\n",
    "    \n",
    "    else:\n",
    "        Data_cleaning_score -= 20\n",
    "        print(\"Score for handling Purity = -20\")\n",
    "    \n",
    "        \n",
    "    return {\n",
    "        \"Missing %\": round(missing_pct, 2),\n",
    "        \"Duplicate %\": round(duplicate_pct, 2),\n",
    "        \"Outlier %\": round(outlier_pct, 2),\n",
    "        \"Purity Score\": max(0, round(purity_score, 2)),\n",
    "        \"Data_cleaning_score\": int(Data_cleaning_score)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "df = pd.read_csv(\"nike_sales_cleaned.csv\")\n",
    "result = dataset_purity(df)\n",
    "\n",
    "for key, value in result.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561771a6-e51c-4a5f-bcf0-22f25200372a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
